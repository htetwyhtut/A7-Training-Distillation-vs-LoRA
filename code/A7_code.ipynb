{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A7: Training Distillation vs LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  0.Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from tqdm import tqdm \n",
    "from peft import PeftModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  1.Toxic Comment Dataset (1 point)\n",
    "\n",
    "Find and load a dataset that includes toxic comments or hate speech. This dataset will be used for training and evaluating the models. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 9000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2970\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Loading Dataset from tweet_eval\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "    \"hate\": (\"text\", None) \n",
    "}\n",
    "\n",
    "task_name = \"hate\"\n",
    "raw_datasets = datasets.load_dataset(\"cardiffnlp/tweet_eval\", task_name)\n",
    "print(raw_datasets)\n",
    "# Expected output: train, validation, test splits with 'text' and 'label' columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'non-hate': 0, 'hate': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract label information\n",
    "label_list = raw_datasets['train'].features['label'].names  # ['non-hate', 'hate']\n",
    "label2id = {v: i for i, v in enumerate(label_list)}\n",
    "id2label = {i: v for v, i in label2id.items()}\n",
    "label2id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'non-hate', 1: 'hate'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2\n"
     ]
    }
   ],
   "source": [
    "# Number of labels \n",
    "num_labels = len(label_list)  # Should be 2\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a1d163f94e4f829539c0dc4341ced5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2970 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset Preprocessing\n",
    "teacher_id = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher_id)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    sentence1_key, sentence2_key = task_to_keys[task_name]\n",
    "    args = (\n",
    "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*args, max_length=128, truncation=True)\n",
    "    return result\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns ('text') and rename 'label' to 'labels'\n",
    "column_dataset = [item for item in task_to_keys[task_name] if item is not None]  # ['text']\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(column_dataset)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Create subsets using full available sizes ## Just use full dataset since it is little\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=1150).select(range(len(tokenized_datasets[\"train\"])))\n",
    "small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=1150).select(range(len(tokenized_datasets[\"validation\"])))\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=1150).select(range(len(tokenized_datasets[\"test\"])))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=64, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=64, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(small_test_dataset, batch_size=64, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  2.Odd Layer vs Even Layer Training (2 points)\n",
    " Based on the case-studies/distilBERT.ipynb, modify as follows:\n",
    "1) Train the student model using the odd layers {1, 3, 5, 7, 9, 11} from the 12-layer teacher to the 6-layer student. (1 point)\n",
    "2) Train the student model using the even layers {2, 4, 6, 8, 10, 12} from the 12-layer teacher to the 6-layer student. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Teacher Model Setup\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_id, \n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "teacher_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student Initialization Function\n",
    "\n",
    "from transformers import BertConfig\n",
    "\n",
    "def initialize_student_model(teacher_model, teacher_layers):\n",
    "    # Get teacher configuration and halve layers\n",
    "    configuration = teacher_model.config.to_dict()\n",
    "    configuration['num_hidden_layers'] = 6\n",
    "    configuration = BertConfig.from_dict(configuration)\n",
    "    \n",
    "    # Create student model\n",
    "    student_model = type(teacher_model)(configuration)\n",
    "    \n",
    "    # Copy embeddings\n",
    "    student_model.bert.embeddings.load_state_dict(teacher_model.bert.embeddings.state_dict())\n",
    "    \n",
    "    # Copy specified teacher layers to student layers\n",
    "    for student_idx, teacher_idx in enumerate(teacher_layers):\n",
    "        student_layer = student_model.bert.encoder.layer[student_idx]\n",
    "        teacher_layer = teacher_model.bert.encoder.layer[teacher_idx]\n",
    "        student_layer.load_state_dict(teacher_layer.state_dict())\n",
    "    \n",
    "    # Copy pooler (if present) and classifier\n",
    "    if hasattr(teacher_model.bert, 'pooler'):\n",
    "        student_model.bert.pooler.load_state_dict(teacher_model.bert.pooler.state_dict())\n",
    "    student_model.classifier.load_state_dict(teacher_model.classifier.state_dict())\n",
    "    \n",
    "    return student_model\n",
    "\n",
    "# Define layer indices  (wrote in 0 based indices)\n",
    "odd_layers = [0, 2, 4, 6, 8, 10]  # 1-based: 1,3,5,7,9,11\n",
    "even_layers = [1, 3, 5, 7, 9, 11]  # 1-based: 2,4,6,8,10,12\n",
    "\n",
    "# Initialize student models\n",
    "student_odd = initialize_student_model(teacher_model, odd_layers).to(device)\n",
    "student_even = initialize_student_model(teacher_model, even_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AIT (DSAI)\\Spring 2025\\NLP\\nlp-env\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/705 [00:00<?, ?it/s]d:\\AIT (DSAI)\\Spring 2025\\NLP\\nlp-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      " 20%|██        | 141/705 [00:38<02:29,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odd Layers - Epoch 1: Train Loss 0.5008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 282/705 [01:19<01:58,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odd Layers - Epoch 2: Train Loss 0.3481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 423/705 [02:00<01:02,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odd Layers - Epoch 3: Train Loss 0.2173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 564/705 [02:40<00:31,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odd Layers - Epoch 4: Train Loss 0.1141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705/705 [03:20<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odd Layers - Epoch 5: Train Loss 0.0645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 140/705 [00:39<02:35,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even Layers - Epoch 1: Train Loss 0.5013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 282/705 [01:18<01:46,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even Layers - Epoch 2: Train Loss 0.3530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 423/705 [01:56<01:11,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even Layers - Epoch 3: Train Loss 0.2310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 564/705 [02:35<00:31,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even Layers - Epoch 4: Train Loss 0.1311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705/705 [03:14<00:00,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even Layers - Epoch 5: Train Loss 0.0835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr = 5e-5\n",
    "\n",
    "def train_student(student_model, name):\n",
    "    optimizer = AdamW(student_model.parameters(), lr=lr)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    )\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        student_model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(**batch)\n",
    "            student_outputs = student_model(**batch)\n",
    "            loss = student_outputs.loss  # Classification loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"{name} - Epoch {epoch+1}: Train Loss {avg_loss:.4f}\")\n",
    "    return train_losses\n",
    "\n",
    "# Train both models\n",
    "odd_losses = train_student(student_odd, \"Odd Layers\")\n",
    "even_losses = train_student(student_even, \"Even Layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  3.LoRA (Low-Rank Adaptation) (1 point)\n",
    " Implement LoRA to train the 12-layer student model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 20%|██        | 141/705 [00:34<02:10,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA - Epoch 1: Train Loss 0.6415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 282/705 [01:09<01:37,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA - Epoch 2: Train Loss 0.5701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 423/705 [01:44<01:04,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA - Epoch 3: Train Loss 0.5315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 565/705 [02:19<00:29,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA - Epoch 4: Train Loss 0.5064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705/705 [02:54<00:00,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA - Epoch 5: Train Loss 0.4949\n"
     ]
    }
   ],
   "source": [
    "#Setup Lora Model\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Initialize 12-layer model\n",
    "student_lora = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_id,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ").to(device)\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "student_lora = get_peft_model(student_lora, lora_config)\n",
    "\n",
    "# Training\n",
    "optimizer = AdamW(student_lora.parameters(), lr=lr)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "lora_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    student_lora.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = student_lora(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    lora_losses.append(avg_loss)\n",
    "    print(f\"LoRA - Epoch {epoch+1}: Train Loss {avg_loss:.4f}\")\n",
    "\n",
    "# Save model\n",
    "student_lora.save_pretrained(\"student_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  4.Evaluation and Analysis (1 point)\n",
    " 1) Evaluate the models on the test set, and analyze the performance of the models trained with Odd Layers, Even Layers, and LoRA. Discuss the differences in performance across the three methods. (0.5 point)\n",
    "2) Discuss the challenges encountered during the implementation, specifically comparing distillation fine-tuning models (Odd and Even Layer) with LoRA fine-tuning. Propose improvements or modifications to address the challenges. (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation Code\n",
    "\n",
    "def evaluate_model(model, name, test_dataloader, device):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Load metrics fresh for each model\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    \n",
    "    # Define loss function (CrossEntropyLoss for classification)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize variables for loss accumulation\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            # Move batch to the correct device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            # Get model outputs\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            labels = batch[\"labels\"]\n",
    "            \n",
    "            # Compute loss for the batch\n",
    "            loss = loss_fn(logits, labels)\n",
    "            batch_loss = loss.item() * len(labels)  # Multiply by batch size\n",
    "            total_loss += batch_loss\n",
    "            total_samples += len(labels)\n",
    "            \n",
    "            # Get predictions (argmax over logits)\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            \n",
    "            # Add batch results to metrics\n",
    "            accuracy_metric.add_batch(predictions=predictions, references=labels)\n",
    "            f1_metric.add_batch(predictions=predictions, references=labels)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / total_samples\n",
    "    \n",
    "    # Compute final accuracy and F1-score\n",
    "    accuracy = accuracy_metric.compute()['accuracy']\n",
    "    f1 = f1_metric.compute()['f1']\n",
    "    \n",
    "    # Return metrics\n",
    "    return avg_loss, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model Type | Test Set Loss | Test Set Accuracy | F1-Score |\n",
      "|------------|---------------|-------------------|----------|\n",
      "| Odd Layer  | 2.7225        | 52.32%           | 0.63    |\n",
      "| Even Layer | 2.2576        | 51.14%           | 0.63    |\n",
      "| LoRA       | 0.8840        | 53.77%           | 0.63    |\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each model\n",
    "odd_loss, odd_acc, odd_f1 = evaluate_model(student_odd, \"Odd Layers\", test_dataloader, device)\n",
    "even_loss, even_acc, even_f1 = evaluate_model(student_even, \"Even Layers\", test_dataloader, device)\n",
    "lora_loss, lora_acc, lora_f1 = evaluate_model(student_lora, \"LoRA\", test_dataloader, device)\n",
    "\n",
    "# Print the table\n",
    "print(\"| Model Type | Test Set Loss | Test Set Accuracy | F1-Score |\")\n",
    "print(\"|------------|---------------|-------------------|----------|\")\n",
    "print(f\"| Odd Layer  | {odd_loss:.4f}        | {odd_acc*100:.2f}%           | {odd_f1:.2f}    |\")\n",
    "print(f\"| Even Layer | {even_loss:.4f}        | {even_acc*100:.2f}%           | {even_f1:.2f}    |\")\n",
    "print(f\"| LoRA       | {lora_loss:.4f}        | {lora_acc*100:.2f}%           | {lora_f1:.2f}    |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./lora_base_model\\\\tokenizer_config.json',\n",
       " './lora_base_model\\\\special_tokens_map.json',\n",
       " './lora_base_model\\\\vocab.txt',\n",
       " './lora_base_model\\\\added_tokens.json',\n",
       " './lora_base_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Model and Data\n",
    "\n",
    "base_model_save_path = \"./lora_base_model\"\n",
    "adapter_save_path = \"./lora_adapter\"\n",
    "base_model = student_lora.base_model  \n",
    "\n",
    "# Save the base model\n",
    "base_model.save_pretrained(base_model_save_path)\n",
    "\n",
    "# Save the LoRA adapter weights and configuration\n",
    "student_lora.save_pretrained(adapter_save_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(base_model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
